{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1182853,"sourceType":"datasetVersion","datasetId":672162}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook is part 2 of [100% accuracy using One-Hot Encoding](https://www.kaggle.com/code/mghobashy/100-accuracy-model-using-one-hot-encoding)\n### In this continuation, we talk about ***Train-Test Contamination*** while exploring different approaches to improve our model.\n# Import the necessary libraries:","metadata":{}},{"cell_type":"code","source":"import random\nimport regex as re\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import TextVectorization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-19T16:53:35.815412Z","iopub.execute_input":"2024-06-19T16:53:35.815777Z","iopub.status.idle":"2024-06-19T16:53:49.382861Z","shell.execute_reply.started":"2024-06-19T16:53:35.815749Z","shell.execute_reply":"2024-06-19T16:53:49.382059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data = pd.read_csv(\"/kaggle/input/disease-symptom-description-dataset/dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:53:49.384465Z","iopub.execute_input":"2024-06-19T16:53:49.385085Z","iopub.status.idle":"2024-06-19T16:53:49.423817Z","shell.execute_reply.started":"2024-06-19T16:53:49.385057Z","shell.execute_reply":"2024-06-19T16:53:49.422732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:53:49.425205Z","iopub.execute_input":"2024-06-19T16:53:49.425614Z","iopub.status.idle":"2024-06-19T16:53:49.458896Z","shell.execute_reply.started":"2024-06-19T16:53:49.425576Z","shell.execute_reply":"2024-06-19T16:53:49.458008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean and preprocess the data:","metadata":{}},{"cell_type":"markdown","source":"## In Part 1, we converted the text data into integers to feed through our neural network. Here, we will explore more advanced strategies like LSTMs and K-means clustering that can work with text.","metadata":{}},{"cell_type":"markdown","source":"### First we concatenate the entire Symptoms columns into a single column.","metadata":{}},{"cell_type":"code","source":"symptom_cols = main_data.columns.difference(['Disease'])  # Select all columns except 'Disease'\n\n# Combine all symptom columns into a single column\nconc_df = main_data.copy()  # Copy the original data incase we needed the original later\nconc_df['Symptoms'] = conc_df[symptom_cols].apply(lambda row: ','.join(row.dropna()), # Dropping NaN\n                                                  axis=1)\n\n# Drop duplicate symptoms within each cell\nconc_df['Symptoms'] = conc_df['Symptoms'].apply(lambda x: ','.join(sorted(set(\n                                                x.split(','))) if x else ''))\n\n# Keep only the 'Disease' and 'Symptoms' columns\nstay_cols = ['Disease', 'Symptoms']\nconc_df = conc_df[stay_cols]\n\nconc_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:53:58.570899Z","iopub.execute_input":"2024-06-19T16:53:58.571970Z","iopub.status.idle":"2024-06-19T16:53:59.049348Z","shell.execute_reply.started":"2024-06-19T16:53:58.571909Z","shell.execute_reply":"2024-06-19T16:53:59.048372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the Symptoms\nconc_df['Symptoms'][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:53:59.257187Z","iopub.execute_input":"2024-06-19T16:53:59.257497Z","iopub.status.idle":"2024-06-19T16:53:59.263159Z","shell.execute_reply.started":"2024-06-19T16:53:59.257471Z","shell.execute_reply":"2024-06-19T16:53:59.262243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Then we clean the 'Symptoms' column.","metadata":{}},{"cell_type":"code","source":"def strip_to_basic_tokens(text):\n    # Remove double spaces and underscores\n    text = re.sub(r'[_\\s]+', ' ', text)\n    # Split by commas and lowercase the tokens\n    tokens = [token.strip().lower() for token in text.split(',')]\n    return tokens\n\ndata = conc_df.copy() # Making a copy\n\n# Apply the function to 'Symptoms' column\ndata['Basic Tokens'] = data['Symptoms'].apply(strip_to_basic_tokens)\ndata['Basic Tokens'] = data['Basic Tokens'].apply(lambda x: ', '.join(x))\ndata = data.drop(['Symptoms'], axis = 1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:00.778026Z","iopub.execute_input":"2024-06-19T16:54:00.778829Z","iopub.status.idle":"2024-06-19T16:54:00.932206Z","shell.execute_reply.started":"2024-06-19T16:54:00.778796Z","shell.execute_reply":"2024-06-19T16:54:00.931382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now the Symptoms column is ready\ndata['Basic Tokens'][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:01.182687Z","iopub.execute_input":"2024-06-19T16:54:01.183331Z","iopub.status.idle":"2024-06-19T16:54:01.189121Z","shell.execute_reply.started":"2024-06-19T16:54:01.183299Z","shell.execute_reply":"2024-06-19T16:54:01.188066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will also label encode the 'Disease' column.","metadata":{}},{"cell_type":"code","source":"# Flatten the 'Disease' column into a single Series\nencoded_data = data.copy()\nflattened_series = encoded_data['Disease'].astype(str)\n# Apply label encoding on the Disease column\nencoder = LabelEncoder()\nencoded_values = encoder.fit_transform(flattened_series)\nencoded_data['Disease'] = encoded_values\n# Saving the encoding to reverse it later in the predictions\nlabel_mapping = {index: label for index, label in enumerate(encoder.classes_)}\n\nencoded_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:02.074930Z","iopub.execute_input":"2024-06-19T16:54:02.075805Z","iopub.status.idle":"2024-06-19T16:54:02.089513Z","shell.execute_reply.started":"2024-06-19T16:54:02.075773Z","shell.execute_reply":"2024-06-19T16:54:02.088371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now that the data is ready, let's review how we are going to train and evaluate our models.","metadata":{}},{"cell_type":"markdown","source":"# Bidirectional LSTM:","metadata":{}},{"cell_type":"markdown","source":"### First we create the train, test, val.","metadata":{}},{"cell_type":"code","source":"train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n    encoded_data[\"Basic Tokens\"].to_numpy(),\n    encoded_data[\"Disease\"].to_numpy(),\n    stratify = data[\"Disease\"], # To make sure of an even distribution between the train and test\n    test_size=0.25,\n    random_state=42)\nval_sentences, test_sentences, val_labels, test_labels = train_test_split(\n    val_sentences,\n    val_labels,\n    test_size=0.5,\n    random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:03.599388Z","iopub.execute_input":"2024-06-19T16:54:03.600132Z","iopub.status.idle":"2024-06-19T16:54:03.615088Z","shell.execute_reply.started":"2024-06-19T16:54:03.600098Z","shell.execute_reply":"2024-06-19T16:54:03.614079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next, we will set up our `text_vectorizer` layer and adapt it.","metadata":{}},{"cell_type":"code","source":"max_length = max(len(sentence.split()) for sentence in train_sentences)\ntext_vectorizer = TextVectorization(split=\"whitespace\", # how to split tokens\n                                    output_mode=\"int\", # how to map tokens to numbers\n                                    output_sequence_length = max_length,\n                                    #standardize=\"lower_and_strip_punctuation\", # We've already done that\n                                   )","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:04.648839Z","iopub.execute_input":"2024-06-19T16:54:04.649530Z","iopub.status.idle":"2024-06-19T16:54:05.434250Z","shell.execute_reply.started":"2024-06-19T16:54:04.649494Z","shell.execute_reply":"2024-06-19T16:54:05.433291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_vectorizer.adapt(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:05.435706Z","iopub.execute_input":"2024-06-19T16:54:05.436027Z","iopub.status.idle":"2024-06-19T16:54:05.562162Z","shell.execute_reply.started":"2024-06-19T16:54:05.436000Z","shell.execute_reply":"2024-06-19T16:54:05.561142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see how the text vectorizer handles the text.","metadata":{}},{"cell_type":"code","source":"random_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nVectorized version:\")\ntext_vectorizer([random_sentence])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:06.702639Z","iopub.execute_input":"2024-06-19T16:54:06.703382Z","iopub.status.idle":"2024-06-19T16:54:07.974467Z","shell.execute_reply.started":"2024-06-19T16:54:06.703349Z","shell.execute_reply":"2024-06-19T16:54:07.973539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the max vocab length\nwords_in_vocab = text_vectorizer.get_vocabulary()\nmax_vocab_length = len(words_in_vocab)\nmax_vocab_length","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:07.976509Z","iopub.execute_input":"2024-06-19T16:54:07.977222Z","iopub.status.idle":"2024-06-19T16:54:07.987121Z","shell.execute_reply.started":"2024-06-19T16:54:07.977170Z","shell.execute_reply":"2024-06-19T16:54:07.985999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the number of targets to put in the output layer\ntargets = data['Disease'].nunique()\ntargets","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:07.988128Z","iopub.execute_input":"2024-06-19T16:54:07.988398Z","iopub.status.idle":"2024-06-19T16:54:07.996566Z","shell.execute_reply.started":"2024-06-19T16:54:07.988373Z","shell.execute_reply":"2024-06-19T16:54:07.995519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now, let's create, compile, and train the model.","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\n# We set the embedding layer\nembedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding\")\n\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM((64)))(x)\nx = layers.Dense(64, activation=\"relu\")(x)\noutputs = layers.Dense(targets, activation=\"softmax\")(x)\nmodel_1 = tf.keras.Model(inputs, outputs, name=\"Bidirectional_LSTM\")","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:09.368976Z","iopub.execute_input":"2024-06-19T16:54:09.369661Z","iopub.status.idle":"2024-06-19T16:54:09.633628Z","shell.execute_reply.started":"2024-06-19T16:54:09.369629Z","shell.execute_reply":"2024-06-19T16:54:09.632679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.compile(loss=\"sparse_categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:10.129539Z","iopub.execute_input":"2024-06-19T16:54:10.129895Z","iopub.status.idle":"2024-06-19T16:54:10.144138Z","shell.execute_reply.started":"2024-06-19T16:54:10.129872Z","shell.execute_reply":"2024-06-19T16:54:10.143187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:14.704910Z","iopub.execute_input":"2024-06-19T16:54:14.705595Z","iopub.status.idle":"2024-06-19T16:54:14.730409Z","shell.execute_reply.started":"2024-06-19T16:54:14.705562Z","shell.execute_reply":"2024-06-19T16:54:14.729458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', \n                               restore_best_weights=True)\n\n\nmodel_1_history = model_1.fit(\n    train_sentences,\n    train_labels,\n    epochs=500,\n    validation_data=(val_sentences, val_labels),\n    callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:15.198888Z","iopub.execute_input":"2024-06-19T16:54:15.199920Z","iopub.status.idle":"2024-06-19T16:54:27.605762Z","shell.execute_reply.started":"2024-06-19T16:54:15.199883Z","shell.execute_reply":"2024-06-19T16:54:27.604994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.evaluate(test_sentences, test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:54:27.607848Z","iopub.execute_input":"2024-06-19T16:54:27.608277Z","iopub.status.idle":"2024-06-19T16:54:27.779664Z","shell.execute_reply.started":"2024-06-19T16:54:27.608242Z","shell.execute_reply":"2024-06-19T16:54:27.778683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Great! 100% accuracy on both train and test. Looks like the model is flawless, but ***is it though?***","metadata":{}},{"cell_type":"markdown","source":"### Let's view the nearest neighbors (symptoms) for each class label (disease). It should make sense *medically* since the model achieved 100% accuracy.","metadata":{}},{"cell_type":"code","source":"# Generate a random number between 0 and 40 since there's 41 targets\nrandom_number = random.randint(0, 40)\n\ndef find_nearest_neighbors(class_label):\n    # Get the embedding layer of the model\n    embedding_layer = model_1.get_layer(\"embedding\")\n\n    # Get the weights of the embedding layer\n    weights = embedding_layer.get_weights()[0]\n\n    # Prepare the nearest neighbors model\n    neighbors_model = NearestNeighbors(n_neighbors=5, algorithm='auto')\n    \n    # Fit the nearest neighbors model with the weights of the embedding layer\n    neighbors_model.fit(weights)\n\n    # Get the embedding vector for the class label\n    class_embedding = weights[class_label]\n\n    # Find the nearest neighbors to the class embedding\n    distances, indices = neighbors_model.kneighbors([class_embedding])\n    print(\"Nearest neighbors to class label '{}':\".format(label_mapping[class_label]))\n    for i, idx in enumerate(indices[0]):\n        print(\"{}: {}\".format(i+1, text_vectorizer.get_vocabulary()[idx]))\n\nfind_nearest_neighbors(class_label=12) # replace '12' with 'random_number' to view the rest","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:00.280672Z","iopub.execute_input":"2024-06-19T16:55:00.281518Z","iopub.status.idle":"2024-06-19T16:55:00.368144Z","shell.execute_reply.started":"2024-06-19T16:55:00.281481Z","shell.execute_reply":"2024-06-19T16:55:00.367143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Well, this is completely wrong. But why is the model behaving poorly? Didn't we achieve 100% accuracy? What happened?\n## We encountered this issue in part 1 when we used LabelEncoder to encode the entire dataset. Despite achieving around 100% accuracy on both training and testing, the model's results were nonsensical when deployed.\n## This happened due to the **\"Train-Test Contamination\"**","metadata":{}},{"cell_type":"markdown","source":"# Train-Test Contamination:\n[Read more about it here](https://towardsdatascience.com/the-dreaded-antagonist-data-leakage-in-machine-learning-5f08679852cc)\n## It's a part of larger concept known as *Data Leakage*.\n### Data leakage occurs when information from the test dataset is mistakenly included in the training dataset.\n### The result? Unrealistically good performance metrics during training, but poor performance when the model is actually put to use.\n### In simpler terms, the model memorized information that it should not have access to, leading to artificially inflated performance metrics during training.","metadata":{}},{"cell_type":"markdown","source":"### But how and when did that happen?\n### Well, if we revisit the data again after knowing about train-test contamination, we will notice a crucial oversight.","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:05.024967Z","iopub.execute_input":"2024-06-19T16:55:05.025693Z","iopub.status.idle":"2024-06-19T16:55:05.035219Z","shell.execute_reply.started":"2024-06-19T16:55:05.025657Z","shell.execute_reply":"2024-06-19T16:55:05.034258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The \"Basic Tokens\" rows for each disease are too similar. This similarity means that when we split the data into train and test sets, both sets ended up containing nearly identical or highly overlapping instances of data.","metadata":{}},{"cell_type":"markdown","source":"#### How can we solve this problem with this particular dataset? While we can't directly fix it, we can approach the data differently. \n#### One method is to concatenate the rows for each disease while emphasizing unique symptoms to give them more weight during model training. \n#### Alternatively, we can employ unsupervised techniques that focus solely on the features. And one of these techniques is...","metadata":{}},{"cell_type":"markdown","source":"# K-means clustering:","metadata":{}},{"cell_type":"markdown","source":"#### Clustering helps mitigate train-test contamination by promoting a generalized learning approach based on clusters. This approach enhances the modelâ€™s ability to generalize and make accurate predictions on new, unseen data points.\n#### Clustering is particularly suitable for this dataset, whether as a preprocessing step or the primary model.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:08.650901Z","iopub.execute_input":"2024-06-19T16:55:08.651292Z","iopub.status.idle":"2024-06-19T16:55:08.660667Z","shell.execute_reply.started":"2024-06-19T16:55:08.651261Z","shell.execute_reply":"2024-06-19T16:55:08.659690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can either use the Keras text vectorizer with an embedding layer to vectorize the text, or we can use TF-IDF.","metadata":{}},{"cell_type":"markdown","source":"## Keras Text Vectorizer and Embedding","metadata":{}},{"cell_type":"code","source":"text_vectorizer","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:10.797871Z","iopub.execute_input":"2024-06-19T16:55:10.798802Z","iopub.status.idle":"2024-06-19T16:55:10.804592Z","shell.execute_reply.started":"2024-06-19T16:55:10.798766Z","shell.execute_reply":"2024-06-19T16:55:10.803524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = text_vectorizer(data['Basic Tokens'])\nX_embed = embedding(X)\nX_embed.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:11.446495Z","iopub.execute_input":"2024-06-19T16:55:11.446859Z","iopub.status.idle":"2024-06-19T16:55:11.519738Z","shell.execute_reply.started":"2024-06-19T16:55:11.446829Z","shell.execute_reply":"2024-06-19T16:55:11.518817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### If we used X_embed we would get this error `ValueError: Found array with dim 3. KMeans expected <= 2`. That's why we need to use `GlobalAverage`.","metadata":{}},{"cell_type":"code","source":"# Apply GlobalAveragePooling1D\nglobal_avg_pooling = layers.GlobalAveragePooling1D(data_format='channels_last')\nX_avg = global_avg_pooling(X_embed).numpy()\nX_avg.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:13.700495Z","iopub.execute_input":"2024-06-19T16:55:13.701438Z","iopub.status.idle":"2024-06-19T16:55:13.714111Z","shell.execute_reply.started":"2024-06-19T16:55:13.701403Z","shell.execute_reply":"2024-06-19T16:55:13.713175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label_encode the disease column.","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(data['Disease'])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:16.746727Z","iopub.execute_input":"2024-06-19T16:55:16.747102Z","iopub.status.idle":"2024-06-19T16:55:16.753052Z","shell.execute_reply.started":"2024-06-19T16:55:16.747073Z","shell.execute_reply":"2024-06-19T16:55:16.752160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we create our K-Means model.","metadata":{}},{"cell_type":"code","source":"n_clusters = 41  # The number of diseases the dataset\nkmeans_1 = KMeans(n_clusters=n_clusters, random_state=42)\ndata['cluster_embed'] = kmeans_1.fit_predict(X_avg)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:20.294971Z","iopub.execute_input":"2024-06-19T16:55:20.295367Z","iopub.status.idle":"2024-06-19T16:55:23.003877Z","shell.execute_reply.started":"2024-06-19T16:55:20.295334Z","shell.execute_reply":"2024-06-19T16:55:23.002985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating Clustering Performance.\n#### ARI and NMI values closer to 1 indicate higher similarity or correlation between the predicted clusters and the true disease labels, while values closer to 0 indicate no meaningful similarity.","metadata":{}},{"cell_type":"code","source":"# Compute Adjusted Rand Index\nari = adjusted_rand_score(y, data['cluster_embed'])\nprint(f'Adjusted Rand Index (ARI): {ari:.4f}')\n\n# Compute Normalized Mutual Information\nnmi = normalized_mutual_info_score(y, data['cluster_embed'])\nprint(f'Normalized Mutual Information (NMI): {nmi:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:23.005398Z","iopub.execute_input":"2024-06-19T16:55:23.006103Z","iopub.status.idle":"2024-06-19T16:55:23.031695Z","shell.execute_reply.started":"2024-06-19T16:55:23.006072Z","shell.execute_reply":"2024-06-19T16:55:23.030721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize the clustering.","metadata":{}},{"cell_type":"code","source":"# Reduce dimensionality for visualization\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_avg)\n\ntsne_df = pd.DataFrame(X_tsne, columns=['TSNE Component 1', 'TSNE Component 2'])\ntsne_df['Cluster'] = data['cluster_embed']\ntsne_df['Disease'] = data['Disease']\n\n# Creating the plot using Plotly Express\nfig = px.scatter(\n    tsne_df, \n    x='TSNE Component 1', \n    y='TSNE Component 2', \n    color='Cluster', \n    hover_data=['Disease'],\n    title='t-SNE Visualization of Symptom Clusters',\n    color_continuous_scale=px.colors.qualitative.Vivid\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:55:31.289405Z","iopub.execute_input":"2024-06-19T16:55:31.290307Z","iopub.status.idle":"2024-06-19T16:55:48.455578Z","shell.execute_reply.started":"2024-06-19T16:55:31.290270Z","shell.execute_reply":"2024-06-19T16:55:48.454614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looks great, but we can do better.","metadata":{}},{"cell_type":"markdown","source":"## TF-IDF\n##### Term Frequency (TF): The frequency of a word in a document.\n##### Inverse Document Frequency (IDF): The rarity of the word across all documents.","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(data['Basic Tokens'])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:56:20.143755Z","iopub.execute_input":"2024-06-19T16:56:20.144475Z","iopub.status.idle":"2024-06-19T16:56:20.244526Z","shell.execute_reply.started":"2024-06-19T16:56:20.144442Z","shell.execute_reply":"2024-06-19T16:56:20.243685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label_encode the disease column.","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(data['Disease'])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:56:21.269838Z","iopub.execute_input":"2024-06-19T16:56:21.270762Z","iopub.status.idle":"2024-06-19T16:56:21.276543Z","shell.execute_reply.started":"2024-06-19T16:56:21.270724Z","shell.execute_reply":"2024-06-19T16:56:21.275559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### And now we train the model","metadata":{}},{"cell_type":"code","source":"n_clusters = 41  # The number of diseases the dataset\nkmeans_2 = KMeans(n_clusters=n_clusters, random_state=42)\ndata['cluster'] = kmeans_2.fit_predict(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:56:22.787693Z","iopub.execute_input":"2024-06-19T16:56:22.788060Z","iopub.status.idle":"2024-06-19T16:56:23.732698Z","shell.execute_reply.started":"2024-06-19T16:56:22.788029Z","shell.execute_reply":"2024-06-19T16:56:23.731789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:56:23.734159Z","iopub.execute_input":"2024-06-19T16:56:23.738751Z","iopub.status.idle":"2024-06-19T16:56:23.754197Z","shell.execute_reply.started":"2024-06-19T16:56:23.738715Z","shell.execute_reply":"2024-06-19T16:56:23.753156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating Clustering Performance.","metadata":{}},{"cell_type":"code","source":"# Compute Adjusted Rand Index\nari = adjusted_rand_score(y, data['cluster'])\nprint(f'Adjusted Rand Index (ARI): {ari:.4f}')\n\n# Compute Normalized Mutual Information\nnmi = normalized_mutual_info_score(y, data['cluster'])\nprint(f'Normalized Mutual Information (NMI): {nmi:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:56:28.284692Z","iopub.execute_input":"2024-06-19T16:56:28.285108Z","iopub.status.idle":"2024-06-19T16:56:28.298648Z","shell.execute_reply.started":"2024-06-19T16:56:28.285074Z","shell.execute_reply":"2024-06-19T16:56:28.297703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Slight Enhancements in Performance","metadata":{}},{"cell_type":"markdown","source":"### Visualizing the Clustering:","metadata":{}},{"cell_type":"code","source":"# Reduce dimensionality for visualization\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X.toarray())\n\n# Create a DataFrame with the t-SNE results and the cluster labels\ntsne_df = pd.DataFrame(X_tsne, columns=['TSNE Component 1', 'TSNE Component 2'])\ntsne_df['Cluster'] = data['cluster']\ntsne_df['Disease'] = data['Disease']\n\n# Interactive plot using Plotly Express\nfig = px.scatter(\n    tsne_df, \n    x='TSNE Component 1', \n    y='TSNE Component 2', \n    color='Cluster', \n    hover_data=['Disease'],\n    title='t-SNE Visualization of Symptom Clusters',\n    color_continuous_scale=px.colors.qualitative.Vivid\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T16:56:32.683414Z","iopub.execute_input":"2024-06-19T16:56:32.684146Z","iopub.status.idle":"2024-06-19T16:56:47.791676Z","shell.execute_reply.started":"2024-06-19T16:56:32.684115Z","shell.execute_reply":"2024-06-19T16:56:47.790698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking much better. Now let's test all the models.","metadata":{}},{"cell_type":"code","source":"def predict_disease(user_input):\n    \n    '''\n    Predict the disease based on the provided symptoms using three different models:\n    LSTM, Embeddings, and TF-IDF.\n\n    Args:\n        user_input (str): A string of symptoms separated by commas, representing the symptoms provided by the user for disease prediction.\n\n    Returns:\n        LSTM_prediction (str): The disease predicted by the LSTM model.\n        predicted_disease_tfidf (str): The disease predicted by the TF-IDF and k-means clustering model.\n        predicted_disease_embed (str): The disease predicted by the Keras vectorizer and embedding model.\n    '''\n    \n    \n    #LSTM\n    user_input_array = np.array([user_input], dtype=object)\n    user_prediction = model_1.predict(user_input_array)\n    LSTM_prediction = label_mapping[user_prediction.argmax()]\n    \n    # Keras vectorizer and embedding\n    user_input_vector = text_vectorizer([user_input])\n    user_input_vector_embed = embedding(user_input_vector)\n    user_input_vector_avg = global_avg_pooling(user_input_vector_embed).numpy()\n    predicted_cluster_embed = kmeans_1.predict(user_input_vector_avg)\n    cluster_to_disease_embed = data.groupby('cluster_embed')['Disease'].apply(lambda x: x.mode()[0]).to_dict()\n    predicted_disease_embed = cluster_to_disease_embed[predicted_cluster_embed[0]]\n    \n    # TF-IDF\n    user_input_tfidf = vectorizer.transform([user_input])\n    predicted_cluster_tfidf = kmeans_2.predict(user_input_tfidf)\n    cluster_to_disease = data.groupby('cluster')['Disease'].apply(lambda x: x.mode()[0]).to_dict()\n    predicted_disease_tfidf = cluster_to_disease[predicted_cluster_tfidf[0]]\n    \n    return LSTM_prediction, predicted_disease_tfidf, predicted_disease_embed\n\n\nuser_input = \"breathlessness,cough\" # These symptoms are taken from Bronchial Asthma symptoms\nLSTM_prediction, predicted_disease_tfidf, predicted_disease_embed = predict_disease(user_input)\nprint(f\"Using LSTM,the predicted disease for the symptoms '{user_input}' is: {LSTM_prediction}\\n\")\nprint(f\"Using Embeddings,the predicted disease for the symptoms '{user_input}' is: {predicted_disease_embed}\\n\")\nprint(f\"Using TF-IDF,the predicted disease for the symptoms '{user_input}' is: {predicted_disease_tfidf}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-19T17:16:18.611089Z","iopub.execute_input":"2024-06-19T17:16:18.611511Z","iopub.status.idle":"2024-06-19T17:16:18.716311Z","shell.execute_reply.started":"2024-06-19T17:16:18.611480Z","shell.execute_reply":"2024-06-19T17:16:18.715277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looks like the TF-IDF model is the most accurate one, but let's test the models even further.","metadata":{}},{"cell_type":"code","source":"def shuffle_tokens(df, num_tokens=None):\n    '''\n    Randomly shuffle and select a subset of tokens (symptoms) from a randomly selected row in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing two columns: 'Disease' and 'Basic Tokens'.\n        num_tokens (int, optional): The number of tokens to select from the shuffled list. If None, a random number of tokens will be selected.\n\n    Returns:\n        disease (str): The disease corresponding to the randomly selected row.\n        shuffled_tokens (str): A string of randomly shuffled and selected tokens (symptoms) from the chosen row.\n\n    '''\n    # Select a random row index\n    idx = np.random.randint(0, len(df))\n\n    # Retrieve disease value\n    disease = df.iloc[idx]['Disease']\n\n    # Retrieve tokens and shuffle\n    tokens_str = df.iloc[idx]['Basic Tokens']\n    tokens_list = [token.strip() for token in tokens_str.split(',')]\n    np.random.shuffle(tokens_list)\n    \n    # Select a random number of tokens if num_tokens is not specified\n    if num_tokens is None:\n        num_tokens = np.random.randint(1, len(tokens_list) + 1)\n    \n    # Randomly select a subset of tokens\n    selected_tokens = np.random.choice(tokens_list, num_tokens, replace=False)\n    shuffled_tokens = ', '.join(selected_tokens)\n\n    return disease, shuffled_tokens\n\ndef predict_disease(user_input):\n    # LSTM prediction\n    user_input_array = np.array([user_input], dtype=object)\n    user_prediction = model_1.predict(user_input_array, verbose = 0)\n    LSTM_prediction = label_mapping[user_prediction.argmax()]\n    \n    # Keras vectorizer and embedding prediction\n    user_input_vector = text_vectorizer([user_input])\n    user_input_vector_embed = embedding(user_input_vector)\n    user_input_vector_avg = global_avg_pooling(user_input_vector_embed).numpy()\n    predicted_cluster_embed = kmeans_1.predict(user_input_vector_avg)\n    cluster_to_disease_embed = data.groupby('cluster_embed')['Disease'].apply(lambda x: x.mode()[0]).to_dict()\n    predicted_disease_embed = cluster_to_disease_embed[predicted_cluster_embed[0]]\n    \n    # TF-IDF prediction\n    user_input_tfidf = vectorizer.transform([user_input])\n    predicted_cluster_tfidf = kmeans_2.predict(user_input_tfidf)\n    cluster_to_disease = data.groupby('cluster')['Disease'].apply(lambda x: x.mode()[0]).to_dict()\n    predicted_disease_tfidf = cluster_to_disease[predicted_cluster_tfidf[0]]\n    \n    return LSTM_prediction, predicted_disease_tfidf, predicted_disease_embed\n\n# Create the DataFrame\nresults = []\n\nfor _ in range(20):\n    disease, shuffled_tokens = shuffle_tokens(data)\n    LSTM_prediction, predicted_disease_tfidf, predicted_disease_embed = predict_disease(shuffled_tokens)\n    \n    results.append({\n        'Shuffled Tokens': shuffled_tokens,\n        'Actual Disease': disease,\n        'LSTM Prediction': LSTM_prediction,\n        'Embedding Prediction': predicted_disease_embed,\n        'TF-IDF Prediction': predicted_disease_tfidf\n    })\n\nresults_df = pd.DataFrame(results)\n\ndef highlight_matches(s):\n    is_match = s == results_df['Actual Disease']\n    return ['background-color: darkgreen' if v else '' for v in is_match]\n\n# Apply the highlighting to the prediction columns\nstyled_df = results_df.style.apply(highlight_matches, subset=['LSTM Prediction', 'Embedding Prediction', 'TF-IDF Prediction'])\n\nstyled_df","metadata":{"execution":{"iopub.status.busy":"2024-06-19T17:31:13.020003Z","iopub.execute_input":"2024-06-19T17:31:13.020730Z","iopub.status.idle":"2024-06-19T17:31:14.898797Z","shell.execute_reply.started":"2024-06-19T17:31:13.020698Z","shell.execute_reply":"2024-06-19T17:31:14.897619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As shown in the table above, the highlighted cells indicate where the predictions match the actual disease, with the TF-IDF model demonstrating the best performance.","metadata":{}},{"cell_type":"markdown","source":"### Did we solve the train-test contamination? ***NO***\n### We simply navigated around that problem without resulting in an overfitted or poor preformance model.","metadata":{}},{"cell_type":"markdown","source":"#### Can this problem be fixed in this particular dataset? As far as I know, it can't be fixed because the issue is inherent to the entire dataset and the way it was made.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n#### In this notebook, we tackled the challenge of train-test contamination, a persistent issue in machine learning. Despite the inherent difficulties posed by the dataset, we explored various approaches to mitigate its impact. By leveraging techniques like TF-IDF and careful model selection, we navigated around the problem while enhancing the model's performance. While the issue remains unsolved due to its dataset-wide nature, our strategies allowed us to build robust models that generalize well to new data. Moving forward, continued exploration and innovation in preprocessing and modeling techniques will be crucial in addressing such challenges effectively.","metadata":{}}]}